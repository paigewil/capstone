---
title: 'An Analysis of Connecticut State Police Traffic Stops: Machine Learning'
author: "Paige Williams"
always_allow_html: yes
output:
  html_document: default
  md_document:
    variant: markdown_github
  pdf_document: default
  word_document: default
---
#### Use Case: 
Using machine learning algorithms as classifiers to create a model to predict if a traffic stop will end in arrest.

#### Method:
The main question we're trying to answer is if we can predict the arrest status of a Connecticut State Police traffic stop given the features in our dataset. More specifically, we'd like to see if any of those features are demographic. To this end, we will be using supervised machine learning with is_arrested as our outcome variable and all others as our predictor variables. Further data wrangling and NA handling will be performed below, however, the final comprehensive list of the 27 predictor variables used is as follows:

* county_name
* driver_gender
* driver_age
* driver_race_raw
* search_conducted
* search_type_raw
* contraband_found
* stop_duration
* violation_count
* violation_raw_Cell.Phone
* violation_raw_Defective.Lights
* violation_raw_Display.of.Plates
* violation_raw_Equipment.Violation
* violation_raw_Moving.Violation
* violation_raw_Other
* violation_raw_Other.Error
* violation_raw_Registration
* violation_raw_Seatbelt
* violation_raw_Speed.Related
* violaiton_raw_Stop.Sign
* violation_raw_Suspended.License
* violation_raw_Traffic.Control.Signla
* violation_raw_Window.Tint
* stop_hour_part_of_day
* stop_season
* stop_dom
* day_of_week

Since is_arrested is a binary variable, we will be testing classification algorithms including CART, Random Forest, and Naive Bayes. Since our data is imbalanced, we will also test a few solutions in combination with our algorithms, including SMOTE, over-sampling, and a Penalty Matrix. Because of the imbalance, accuracy is not the best metric to evaluate the success of our algorithms. Instead, a combination of Kappa and AUC will be used. 

#### Data Wrangling:
Load in necessary libraries and datasets:
```{r, warning = FALSE, results = 'hide', message = FALSE}
library(rpart)
library(rpart.plot)
library(caret)
library(e1071)
library(caTools)
library(lubridate)
library(DMwR)
library(randomForest)
library(ROSE)
library(dplyr)
library(party)
library(irr)
library(klaR)
library(ROCR)
library(Boruta)
library(caretEnsemble)
library(gbm)

# Capstone data
stops_split <- read.csv("E:/Learning/Springboard Intro to Data Science/capstone/CT_cleaned_split.csv")

stops_split_col <- stops_split[,c(3,4,6,10,12,13,15,17,18,20,21,22,23,24,27,29:42)]
stops_split_col$stop_date <- as.POSIXct(stops_split_col$stop_date, "%Y-%m-%d", tz = "America/New_York")
```
Here, we used the split dataframe, which one hot-encoded the violation_raw column. This makes it easier to use the violation_raw information in a machine learning algorithm.

We need to do some data wrangling before we can apply the machine learning algorithms. For example, we need to regroup factor features so that they don't have too many levels, since some algorithms can't handle high-level factors. We also need to split up some features, extracting seperate pieces of information into new features: 
```{r, results = 'hide', message = FALSE}
# Making new attributes for ML algorithms
stops_split_edit <- stops_split_col

# making hour attribute
stops_split_edit$stop_time_hour <- stops_split_edit$stop_time
stops_split_edit$stop_time_hour[stops_split_edit$stop_time_hour == "0:00"] <- NA
stops_split_edit$stop_time_hour <- sub(":.*", "", stops_split_edit$stop_time_hour)

# grouping hour into times of day since 24 levels might be too granular
stops_split_edit$stop_time_hour_numeric <- as.numeric(stops_split_edit$stop_time_hour)
stops_split_edit$stop_hour_part_of_day <- vector(mode = "character", length = nrow(stops_split_edit))
stops_split_edit$stop_hour_part_of_day[stops_split_edit$stop_time_hour_numeric >= 0 & stops_split_edit$stop_time_hour_numeric < 6] <- "time_block1"
stops_split_edit$stop_hour_part_of_day[stops_split_edit$stop_time_hour_numeric >= 6 & stops_split_edit$stop_time_hour_numeric < 12] <- "time_block2"
stops_split_edit$stop_hour_part_of_day[stops_split_edit$stop_time_hour_numeric >= 12 & stops_split_edit$stop_time_hour_numeric < 18] <- "time_block3"
stops_split_edit$stop_hour_part_of_day[stops_split_edit$stop_time_hour_numeric >= 18 & stops_split_edit$stop_time_hour_numeric <= 23] <- "time_block4"
# -> replacing blanks with NAs
stops_split_edit$stop_hour_part_of_day[stops_split_edit$stop_hour_part_of_day == ""] <- NA
# -> turning into factor
stops_split_edit$stop_hour_part_of_day <- factor(stops_split_edit$stop_hour_part_of_day)
```

```{r, results = 'hide', message = FALSE}
# making stop month attribute 
stops_split_edit$stop_month <- month(stops_split_edit$stop_date)

# -> making seasons variable since month variable might be too granular
stops_split_edit$stop_month_numeric <- as.numeric(stops_split_edit$stop_month)
stops_split_edit$stop_season <- vector(mode = "character", length = nrow(stops_split_edit))
stops_split_edit$stop_season[stops_split_edit$stop_month_numeric >= 3 & stops_split_edit$stop_month_numeric <= 5] <- "spring"
stops_split_edit$stop_season[stops_split_edit$stop_month_numeric >= 6 & stops_split_edit$stop_month_numeric <= 8] <- "summer"
stops_split_edit$stop_season[stops_split_edit$stop_month_numeric >= 9 & stops_split_edit$stop_month_numeric <= 11] <- "autumn"
stops_split_edit$stop_season[stops_split_edit$stop_month_numeric == 12 | stops_split_edit$stop_month_numeric == 1 | stops_split_edit$stop_month_numeric == 2] <- "winter"
stops_split_edit$stop_season <- factor(stops_split_edit$stop_season)
```

```{r, results = 'hide', message = FALSE}
# -> making stop day of month attribute
a <- c("01", "02", "03", "04", "05", "06", "07", "08", "09")
b <- as.character(10:31)
days_label <- c(a, b)
stops_split_edit$day_of_month <- format(stops_split_edit$stop_date, "%d")

# -> breaking day of month in rough thirds since stats investigation showed the last third of the month has a lower arrest/stop ratio
  #1-10
  #11-20
  #21-31
stops_split_edit$day_of_month_numeric <- as.numeric(stops_split_edit$day_of_month)
stops_split_edit$stop_dom <- vector(mode = "character", length = nrow(stops_split_edit))
stops_split_edit$stop_dom[stops_split_edit$day_of_month_numeric >= 1 & stops_split_edit$day_of_month_numeric <= 10] <- "first_third"
stops_split_edit$stop_dom[stops_split_edit$day_of_month_numeric >= 11 & stops_split_edit$day_of_month_numeric <= 20] <- "second_third"
stops_split_edit$stop_dom[stops_split_edit$day_of_month_numeric >= 21 & stops_split_edit$day_of_month_numeric <= 31] <- "third_third"
stops_split_edit$stop_dom <- factor(stops_split_edit$stop_dom)
```

```{r, results = 'hide', message = FALSE}
# add day of week
stops_split_edit$day_of_week <- weekdays(stops_split_edit$stop_date)
stops_split_edit$day_of_week <- factor(stops_split_edit$day_of_week)
```

Cleaning up the dataset from all the wrangling steps:
```{r, results = 'hide', message = FALSE}
# Trim off attributes created to create other attributes
# Also remove Officer ID since we don't want to create an algorithm using an ID column
# Removing original attributes that grouped/aggregated:
  # -> stop_time
  # -> stop_date
  # -> violation_raw
stops_split_small <- stops_split_edit[, c(-1,-2, -7, -13, -30, -31, -33, -34, -36, -37)]
```

We need to turn some features into factors for easier handling in the machine learning algorithms:
```{r, results = 'hide', message = FALSE}
for (i in c(5, 7, 9, 11:25)){
  stops_split_small[[i]] <- as.factor(stops_split_small[[i]])
}
```

We need to handle NAs in the dataset explicitly instead of leaving it up to the machine learning algorithms. We will do so in a way to ensure minimum loss of information:
```{r, results = 'hide', message = FALSE}
summary(stops_split_small)
# NAs
  # -> county_name
  # -> driver_age
  # -> search_type_raw
  # -> stop_hour_part_of_day
  # -> is_arrested

stops_clean <- stops_split_small
```

We will populate driver_age NAs with the median because it ends up being a whole number (like our other driver_age ages) and is very similar to the mean:
```{r, results = 'hide', message = FALSE}
#driver_age
med <- median(stops_clean$driver_age, na.rm = TRUE)
stops_clean$driver_age[is.na(stops_clean$driver_age) == TRUE] <- med
```

For search_type_raw, when no search was conducted, the value is NA. Instead, we'll create a new factor level, "None". In addition, there are some NAs where a search was conducted. Those are bucketed into the pre-existing "Other" category.
```{r, results = 'hide', message = FALSE}
#search_type_raw
# -> replacing blanks with NAs
stops_clean$search_type_raw[stops_clean$search_type_raw == ""] <- NA
table(stops_clean$search_type_raw, stops_clean$search_conducted, useNA = "always")
# -> 313337 are from search_conducted = FALSE
# -> 486 where search_conducted = TRUE but no search type indicated
# adding level to factor to make a "None" level for the search types where no search was conducted
stops_clean$search_type_raw <- factor(stops_clean$search_type_raw, levels = c(levels(stops_clean$search_type_raw), "None"))
stops_clean$search_type_raw[(is.na(stops_clean$search_type_raw) == TRUE & stops_clean$search_conducted == FALSE)] <- "None"

table(stops_clean$search_type_raw, stops_clean$is_arrested, useNA = "always")
# 151/7312  = 0.02065098 are is_arrested == TRUE, so can't remove with out 
#                        significant loss of information
# => will identify as "Other"" since that is the mode of is_arrested = TRUE and a 
#    generic "catch all" level
stops_clean$search_type_raw[(is.na(stops_clean$search_type_raw) == TRUE)] <- "Other"
```

county_name and stop_hour_part_of_day have small amounts of NAs so those rows will be removed since there is no sensible way to populate them:
```{r, results = 'hide', message = FALSE}
# county_name
stops_clean$county_name[stops_clean$county_name == ""] <- NA
#get rid of blank level
stops_clean$county_name <- factor(stops_clean$county_name, levels = levels(stops_clean$county_name)[2:9])
table(stops_clean$county_name, stops_clean$is_arrested, useNA = "always")
# 4/7312 = .000547046 (percentage removed from is_arrested == TRUE)
# 10/306001 = 3.267963e-05
# Not a sizeable removal amount

# stop_hour_part_of_day
table(stops_clean$stop_hour_part_of_day, stops_clean$is_arrested, useNA = "always")
table(stops_clean$stop_hour_part_of_day, stops_clean$county_name, useNA = "always")
# -> no overlapping NAs with county_name
# 8/7308 = 0.001094691 (percentage removed from is_arrested == TRUE after remove
#                       county_name NAs)
# 212/305991 = 0.0006928308

# Since neither county_name nor stop_hour_part_of_day NAs account for a significant
# percentage of our outcome variables, we will remove the rows with those NAs.
stops_clean2 <- stops_clean[!is.na(stops_clean$stop_hour_part_of_day),]
stops_clean2 <- stops_clean2[!is.na(stops_clean2$county_name),]
```

There are NAs in our outcome variable, is_arrested. Since our algorithm might become biased if we assume an arrest status for the NAs, we must remove the rows where is_arrested = NA:
```{r, results = 'hide', message = FALSE}
# How are NAs distributed throughout is_arrested?
table(stops_clean2$is_arrested, useNA = "always")
# 5352 NAs 

stops_clean2 <- stops_clean2[!is.na(stops_clean2$is_arrested),]
stops_clean3 <- stops_clean2
```

Before we start running some algorithms, we need to remove the stop_outcome variable to only focus on predicting is_arrested:
```{r, results = 'hide', message = FALSE}
stops_arrested <- stops_clean3[,-8]
```

#### Algorithms:
Let's split our dataset into a test and training set:
```{r, message = FALSE}
# Since I have a lot of data, I want to make my testing sample a little larger
# to have better tests to make sure I'm choosing the best algorithm

# Splitting data using createDataPartition since it works better with unbalanced data
set.seed(3000)
spl_cdp = createDataPartition(stops_arrested$is_arrested, p = 0.75, list = FALSE)
train_arrest = stops_arrested[spl_cdp,]
test_arrest = stops_arrested[-spl_cdp,]
```

One hurdle in running algorithms with the dataset is that it is signficantly imbalanced, meaning we have a lot more non-arrests than arrests. Specifically, non-arrests make up 97.67% of the data. To account for the imbalance such that the machine learning algorithm doesn't predict False for every observation to get a high accuracy, we will test a number of different imbalance handling solutions with a number of algorithms to pick the best performing model.

As a better measure of how good the model is at predicting the correct outcome, we will look at a combination of Kappa and AUC over accuracy because they are better measures for imbalanced data.

Using SMOTE to fix imbalance with the CART algorithm:
```{r, message = FALSE}
set.seed(111)
train.smote <- SMOTE(is_arrested ~., data = train_arrest)
tree.smote <- rpart(is_arrested ~ ., data = train.smote)
prp(tree.smote)
print(tree.smote)
```

When predicting on the test set, we see that the Kappa is low at 0.2214208 with the AUC, not bad at 0.77:
```{r, message = FALSE}
pred.tree.smote2 <- predict(tree.smote, newdata = test_arrest, type = "class")
confusionMatrix(table(test_arrest$is_arrested, pred.tree.smote2))$overall['Kappa']
roc.curve(test_arrest$is_arrested, pred.tree.smote2)
```

Using SMOTE with Random Forest:
```{r, message = FALSE}
set.seed(111)
rf.smote <- randomForest(is_arrested ~ ., data = train.smote)
pred.rf.smote2 <- predict(rf.smote, newdata = test_arrest, type = "class")
confusionMatrix(table(test_arrest$is_arrested, pred.rf.smote2))$overall['Kappa']
roc.curve(test_arrest$is_arrested, pred.rf.smote2)
```
Here, we get a Kappa of 0.2812 and an AUC of 0.762, overall, not much different than SMOTE with CART.

Using over-sampling to fix imbalance with the Random Forest algorithm:
```{r, message = FALSE}
# 229335 is the number of is_arrested == FALSE in train_arrest
set.seed(111)
train_arrest_over <- ovun.sample(is_arrested ~., data = train_arrest, method = "over", N = 2*229335)$data
rf.over <- randomForest(is_arrested ~ ., data = train_arrest_over, ntree = 50)
pred.rf.over <- predict(rf.over, newdata = test_arrest, type = "class")
confusionMatrix(table(test_arrest$is_arrested, pred.rf.over))$overall['Kappa']
roc.curve(test_arrest$is_arrested, pred.rf.over)
```
The Kappa is slightly higher, but still low at 0.322508 and the AUC is not bad at 0.732:


Using SMOTE to fix imbalance with the Naive Bayes algorithm:
```{r, message = FALSE}
set.seed(111)
nb.model.smote <- naiveBayes(is_arrested ~., data = train.smote, laplace = 1)
nb.predict.smote <- predict(nb.model.smote, newdata = test_arrest, type = "class")
confusionMatrix(table(test_arrest$is_arrested, nb.predict.smote))$overall['Kappa']
roc.curve(test_arrest$is_arrested, nb.predict.smote)
```
The Kappa is still low at 0.256684 with the AUC at 0.744.



Now, instead of balancing the unbalanced dataset by adding manufactured obserations or removing observations, we can also try using a Penalty Matrix. Here, we want a higher penalty for false negatives, i.e., where the model labels an observation as having no arrest when there was one, because we're most concerned about accurately labeling arrests. Thus, our Penalty Matrix will weight those error more heavily.
```{r, results = 'hide', message = FALSE}
PenaltyMatrix = matrix(c(0,1,6,0), byrow = TRUE, nrow = 2)
```

Here, we use CART with our Penalty Matrix:
```{r, message = FALSE}
set.seed(111)
stops_arrest_tree_penalty <- rpart(is_arrested ~., data = train_arrest, parms = list(loss = PenaltyMatrix))
prp(stops_arrest_tree_penalty)
print(stops_arrest_tree_penalty)
PredictCARTPenalty2 = predict(stops_arrest_tree_penalty, newdata = test_arrest, type = "class")
confusionMatrix(table(test_arrest$is_arrested, PredictCARTPenalty2))$overall['Kappa']
roc.curve(test_arrest$is_arrested, PredictCARTPenalty2)
```
So far, this model performs the best with a Kappa at 0.3537442 and an AUC at 0.693.

A complete testing of the combinations of the imbalance handling options (SMOTE, ROSE, over-sampling, under-sampling, and Penalty Matrix) and algorithms (CART, Random Forest, and Naive Bayes) were performed in a seperate R script. For a full investigation, please see the script [here](https://github.com/paigewil/capstone/blob/master/machine_learning.R). The top performing combinations were explored above.


To help us widdle down the model options further, we'll perform a 10-fold CV test on these top performing imbalance solutions and algorithm combinations and see what Kappa and AUC values are returned.

SMOTE with CART:
```{r, message = FALSE, fig.show= 'hide'}
set.seed(123)
folds <- createFolds(stops_arrested$is_arrested, k = 10)
cv_results <- lapply(folds, function(x) {
  arrest_train <- stops_arrested[-x, ]
  arrest_test <- stops_arrested[x, ]
  arrest.train.smote <- SMOTE(is_arrested ~., data = arrest_train)
  tree.model <- rpart(is_arrested ~ ., data = arrest.train.smote)
  arrest.pred <- predict(tree.model, arrest_test, type = "class")
  auc_val <- roc.curve(arrest_test$is_arrested, arrest.pred)$auc
  kappa <- kappa2(data.frame(arrest_test$is_arrested, arrest.pred))$value
  return(c(auc_val, kappa))
})

df_cv_results <- data.frame(cv_results)
#AUC
mean(unlist(df_cv_results[1,]))
#Kappa
mean(unlist(df_cv_results[2,]))
```

Penalty Matrix with CART:
```{r, message = FALSE, fig.show= 'hide'}
set.seed(123)
cv_resultsPM <- lapply(folds, function(x) {
  arrest_train <- stops_arrested[-x, ]
  arrest_test <- stops_arrested[x, ]
  tree.model <- rpart(is_arrested ~ ., data = arrest_train, parms = list(loss = PenaltyMatrix))
  arrest.pred <- predict(tree.model, arrest_test, type = "class")
  auc_val <- roc.curve(arrest_test$is_arrested, arrest.pred)$auc
  kappa <- kappa2(data.frame(arrest_test$is_arrested, arrest.pred))$value
  return(c(auc_val, kappa))
})

df_cv_resultsPM <- data.frame(cv_resultsPM)
#AUC
mean(unlist(df_cv_resultsPM[1,]))
#Kappa
mean(unlist(df_cv_resultsPM[2,]))
```

SMOTE with RandomForest:
```{r, message = FALSE, fig.show= 'hide'}
set.seed(123)
cv_results_rf <- lapply(folds, function(x) {
  arrest_train <- stops_arrested[-x, ]
  arrest_test <- stops_arrested[x, ]
  arrest.train.smote <- SMOTE(is_arrested ~., data = arrest_train)
  tree.model <- randomForest(is_arrested ~ ., data = arrest.train.smote, ntree = 50)
  arrest.pred <- predict(tree.model, arrest_test, type = "class")
  auc_val <- roc.curve(arrest_test$is_arrested, arrest.pred)$auc
  kappa <- kappa2(data.frame(arrest_test$is_arrested, arrest.pred))$value
  return(c(auc_val, kappa))
})

df_cv_results_rf <- data.frame(cv_results_rf)
#AUC
mean(unlist(df_cv_results_rf[1,]))
#Kappa
mean(unlist(df_cv_results_rf[2,]))
```

Over-sampling with RandomForest:
```{r, results = 'hide', message = FALSE, fig.show= 'hide'}
set.seed(123)
f1.train <- stops_arrested[-folds$Fold01, ]
f1.test <- stops_arrested[folds$Fold01, ]
f1.ovun <- ovun.sample(is_arrested ~., data = f1.train, method = "over", N = 2*nrow(f1.train[f1.train$is_arrested == FALSE,]))$data
f1.model <- randomForest(is_arrested ~ ., data = f1.ovun, ntree = 30)
f1.pred <- predict(f1.model, f1.test, type = "class")
f1_auc_val <- roc.curve(f1.test$is_arrested, f1.pred)$auc
f1_kappa <- kappa2(data.frame(f1.test$is_arrested, f1.pred))$value

f2.train <- stops_arrested[-folds$Fold02, ]
f2.test <- stops_arrested[folds$Fold02, ]
f2.ovun <- ovun.sample(is_arrested ~., data = f2.train, method = "over", N = 2*nrow(f2.train[f2.train$is_arrested == FALSE,]))$data
f2.model <- randomForest(is_arrested ~ ., data = f2.ovun, ntree = 30)
f2.pred <- predict(f2.model, f2.test, type = "class")
f2_auc_val <- roc.curve(f2.test$is_arrested, f2.pred)$auc
f2_kappa <- kappa2(data.frame(f2.test$is_arrested, f2.pred))$value

f3.train <- stops_arrested[-folds$Fold03, ]
f3.test <- stops_arrested[folds$Fold03, ]
f3.ovun <- ovun.sample(is_arrested ~., data = f3.train, method = "over", N = 2*nrow(f3.train[f3.train$is_arrested == FALSE,]))$data
f3.model <- randomForest(is_arrested ~ ., data = f3.ovun, ntree = 30)
f3.pred <- predict(f3.model, f3.test, type = "class")
f3_auc_val <- roc.curve(f3.test$is_arrested, f3.pred)$auc
f3_kappa <- kappa2(data.frame(f3.test$is_arrested, f3.pred))$value

f4.train <- stops_arrested[-folds$Fold04, ]
f4.test <- stops_arrested[folds$Fold04, ]
f4.ovun <- ovun.sample(is_arrested ~., data = f4.train, method = "over", N = 2*nrow(f4.train[f4.train$is_arrested == FALSE,]))$data
f4.model <- randomForest(is_arrested ~ ., data = f4.ovun, ntree = 30)
f4.pred <- predict(f4.model, f4.test, type = "class")
f4_auc_val <- roc.curve(f4.test$is_arrested, f4.pred)$auc
f4_kappa <- kappa2(data.frame(f4.test$is_arrested, f4.pred))$value

f5.train <- stops_arrested[-folds$Fold05, ]
f5.test <- stops_arrested[folds$Fold05, ]
f5.ovun <- ovun.sample(is_arrested ~., data = f5.train, method = "over", N = 2*nrow(f5.train[f5.train$is_arrested == FALSE,]))$data
f5.model <- randomForest(is_arrested ~ ., data = f5.ovun, ntree = 30)
f5.pred <- predict(f5.model, f5.test, type = "class")
f5_auc_val <- roc.curve(f5.test$is_arrested, f5.pred)$auc
f5_kappa <- kappa2(data.frame(f5.test$is_arrested, f5.pred))$value

f6.train <- stops_arrested[-folds$Fold06, ]
f6.test <- stops_arrested[folds$Fold06, ]
f6.ovun <- ovun.sample(is_arrested ~., data = f6.train, method = "over", N = 2*nrow(f6.train[f6.train$is_arrested == FALSE,]))$data
f6.model <- randomForest(is_arrested ~ ., data = f6.ovun, ntree = 30)
f6.pred <- predict(f6.model, f6.test, type = "class")
f6_auc_val <- roc.curve(f6.test$is_arrested, f6.pred)$auc
f6_kappa <- kappa2(data.frame(f6.test$is_arrested, f6.pred))$value

f7.train <- stops_arrested[-folds$Fold07, ]
f7.test <- stops_arrested[folds$Fold07, ]
f7.ovun <- ovun.sample(is_arrested ~., data = f7.train, method = "over", N = 2*nrow(f7.train[f7.train$is_arrested == FALSE,]))$data
f7.model <- randomForest(is_arrested ~ ., data = f7.ovun, ntree = 30)
f7.pred <- predict(f7.model, f7.test, type = "class")
f7_auc_val <- roc.curve(f7.test$is_arrested, f7.pred)$auc
f7_kappa <- kappa2(data.frame(f7.test$is_arrested, f7.pred))$value

f8.train <- stops_arrested[-folds$Fold08, ]
f8.test <- stops_arrested[folds$Fold08, ]
f8.ovun <- ovun.sample(is_arrested ~., data = f8.train, method = "over", N = 2*nrow(f8.train[f8.train$is_arrested == FALSE,]))$data
f8.model <- randomForest(is_arrested ~ ., data = f8.ovun, ntree = 30)
f8.pred <- predict(f8.model, f8.test, type = "class")
f8_auc_val <- roc.curve(f8.test$is_arrested, f8.pred)$auc
f8_kappa <- kappa2(data.frame(f8.test$is_arrested, f8.pred))$value

f9.train <- stops_arrested[-folds$Fold09, ]
f9.test <- stops_arrested[folds$Fold09, ]
f9.ovun <- ovun.sample(is_arrested ~., data = f9.train, method = "over", N = 2*nrow(f9.train[f9.train$is_arrested == FALSE,]))$data
f9.model <- randomForest(is_arrested ~ ., data = f9.ovun, ntree = 30)
f9.pred <- predict(f9.model, f9.test, type = "class")
f9_auc_val <- roc.curve(f9.test$is_arrested, f9.pred)$auc
f9_kappa <- kappa2(data.frame(f9.test$is_arrested, f9.pred))$value

f10.train <- stops_arrested[-folds$Fold10, ]
f10.test <- stops_arrested[folds$Fold10, ]
f10.ovun <- ovun.sample(is_arrested ~., data = f10.train, method = "over", N = 2*nrow(f10.train[f10.train$is_arrested == FALSE,]))$data
f10.model <- randomForest(is_arrested ~ ., data = f10.ovun, ntree = 30)
f10.pred <- predict(f10.model, f10.test, type = "class")
f10_auc_val <- roc.curve(f10.test$is_arrested, f10.pred)$auc
f10_kappa <- kappa2(data.frame(f10.test$is_arrested, f10.pred))$value


combined_auc <- c(f1_auc_val, f2_auc_val, f3_auc_val, f4_auc_val, f5_auc_val, f6_auc_val, f7_auc_val, f8_auc_val, f9_auc_val, f10_auc_val)
combined_kappa <- c(f1_kappa, f2_kappa, f3_kappa, f4_kappa, f5_kappa, f6_kappa, f7_kappa, f8_kappa, f9_kappa, f10_kappa)
```

```{r, message = FALSE}
#AUC
mean(combined_auc)
#Kappa
mean(combined_kappa)
```

SMOTE with Naive Bayes:
```{r, message = FALSE, fig.show= 'hide'}
set.seed(123)
cv_results_nb <- lapply(folds, function(x) {
  arrest_train <- stops_arrested[-x, ]
  arrest_test <- stops_arrested[x, ]
  arrest.train.smote <- SMOTE(is_arrested ~., data = arrest_train)
  tree.model <- naiveBayes(is_arrested ~., data = arrest.train.smote, laplace = 1)
  arrest.pred <- predict(tree.model, arrest_test, type = "class")
  auc_val <- roc.curve(arrest_test$is_arrested, arrest.pred)$auc
  kappa <- kappa2(data.frame(arrest_test$is_arrested, arrest.pred))$value
  return(c(auc_val, kappa))
})

df_cv_results_nb <- data.frame(cv_results_nb)
#AUC
mean(unlist(df_cv_results_nb[1,]))
#Kappa
mean(unlist(df_cv_results_nb[2,]))
```

From the resulting Kappas and AUCs, we fine-tuned the parameters of the top three performing models using cross-validation and picked the top performing of them to use as our overall model. To save on performance overhead, full code for the other two contenders, SMOTE with Random Forest and over-sampling with Random Forest can be seen in the [machine learning script](https://github.com/paigewil/capstone/blob/master/machine_learning.R). 

Here is the fine-tuning procedure done on the top performing model, Penalty Matrix with CART:
```{r, message = FALSE, fig.show= 'hide'}
ctrl_cart2 <- trainControl(method = "cv", number = 10)
grid_cart2 <- expand.grid(.cp = seq(0.01,0.1,0.01))
set.seed(13)
m_cart2 <- train(is_arrested ~., data = train_arrest,
                method = "rpart",
                metric = "Kappa",
                trControl = ctrl_cart2,
                tuneGrid = grid_cart2,
                parms = list(loss = PenaltyMatrix))
m_cart2

predict_cart2 <- predict(m_cart2, newdata = test_arrest)
kappa2(data.frame(test_arrest$is_arrested, predict_cart2))$value
roc.curve(test_arrest$is_arrested, predict_cart2)
```

From the Kappa and AUC results, we determined that the best performing algorithm was between SMOTE with Random Forest and CART with Penalty Matrix. Given that CART with Penalty Matrix can be modeled without manufacturing observations, this is the model of choice. However, the Kappa is still very low, indicating this is not a great performing model in its predictive power of arrest status. In an attempt to improve performance, we will explore feature selection and ensemble methods. Let’s start with feature selection.

Here, we use Recursive Feature Elimination, RFE, to identify the most important features to try and reduce our dataset to the most important information. RFE is an algorithm that uses Random Forest to remove redundant features. In the example below, we use Penalty Matrix to handle our imbalanced dataset.

```{r, message = FALSE}
control_fs <- rfeControl(functions = rfFuncs,
                         method = "cv",
                         number = 10)
set.seed(444)
feature_selection_arrest2 <- rfe(is_arrested ~.,
                                     data = train_arrest,
                                     rfeControl = control_fs,
                                     metric = "Kappa",
                                     ntree = 100,
                                     parms = list(loss = PenaltyMatrix))

feature_selection_arrest2
```

The top variables returned are stop_duration, stop_hour_part_of_day, and violation_raw_Other.

Let’s get a more holistic picture by looking at the important features from our parameter fine-tuned Random Forest with SMOTE, Random Forest with over-sampling, and CART with Penalty Matrix algorithms. 

Random Forest with SMOTE and mtry = 3:
```{r, message = FALSE}
set.seed(444)
train_arrest_smote <- SMOTE(is_arrested ~., data = train_arrest)
model.smote.rf <- randomForest(is_arrested ~ ., data = train_arrest_smote, mtry = 3)
pred.rf.smote <- predict(model.smote.rf, newdata = test_arrest, type = "class")
confusionMatrix(table(test_arrest$is_arrested, pred.rf.smote))$overall['Kappa']
roc.curve(test_arrest$is_arrested, pred.rf.smote)
randomForest::varImpPlot(model.smote.rf)
```

Random Forest with over-sampling and mtry = 27:
```{r, message = FALSE}
set.seed(555)
model.over.rf <- randomForest(is_arrested ~ ., data = train_arrest_over, mtry = 27, ntree = 30)
pred.rf.over.finetuned <- predict(model.over.rf, newdata = test_arrest, type = "class")
confusionMatrix(table(test_arrest$is_arrested, pred.rf.over.finetuned))$overall['Kappa']
roc.curve(test_arrest$is_arrested, pred.rf.over.finetuned)
varImpPlot(model.over.rf)
```

CART with Penalty Matrix and cp = 0.01 (*):
```{r, message = FALSE}
cart.penalty.ft <- rpart(is_arrested ~ ., data = train_arrest, cp = 0.01, parms = list(loss = PenaltyMatrix))
pred.cart.penalty.ft <- predict(cart.penalty.ft, newdata = test_arrest, type = "class")
confusionMatrix(table(test_arrest$is_arrested, pred.cart.penalty.ft))$overall['Kappa']
roc.curve(test_arrest$is_arrested, pred.cart.penalty.ft)
prp(cart.penalty.ft)
print(cart.penalty.ft)
```

Based on the results of the algorithms and feature selection, some of the top variables are:

* stop_duration
* search_conducted
* search_type_raw
* contraband_found
* stop_hour_part_of_day

Let’s try to improve our model by reducing our dataset to the aforementioned features.
```{r, message = FALSE}
stops_arrested_top_combo <- stops_arrested[, c(5:9, 25)]

set.seed(133)
spl_top_combo = createDataPartition(stops_arrested_top_combo$is_arrested, p = 0.75, list = FALSE)
train_arrest_top_combo = stops_arrested_top_combo[spl_top_combo,]
test_arrest_top_combo = stops_arrested_top_combo[-spl_top_combo,]

stops_arrest_top_pm_combo <- rpart(is_arrested ~., data = train_arrest_top_combo, cp = 0.01, parms = list(loss = PenaltyMatrix))
predict_top_pm_combo = predict(stops_arrest_top_pm_combo, newdata = test_arrest_top_combo, type = "class")
confusionMatrix(table(test_arrest_top_combo$is_arrested, predict_top_pm_combo))$overall['Kappa']
roc.curve(test_arrest_top_combo$is_arrested, predict_top_pm_combo)
```
As we can see from the resulting Kappa and AUC, our performance isn’t really improved through feature selection. Let’s explore the ensemble method to try and combine the results of the different algorithms in another attempt to improve model performance.  

Here we will use a combination of CART, Random Forest, and Naïve Bayes with the Penalty Matrix. The predictions of each algorithm will be combined using a majority vote.

```{r, message = FALSE}
set.seed(111)
stops_arrest_rf_penalty <- randomForest(is_arrested ~., data = train_arrest, ntree= 100, parms = list(loss = PenaltyMatrix))
set.seed(111)
stops_arrest_nb_penalty <- naiveBayes(is_arrested ~., data = train_arrest, laplace = 1, parms = list(loss = PenaltyMatrix))

set.seed(123)
train_arrest_ensemble <- train_arrest
test_arrest_ensemble <- test_arrest
train_arrest_ensemble$rf_ensemble <- predict(stops_arrest_rf_penalty, type = "prob")[,2]
test_arrest_ensemble$rf_ensemble <- predict(stops_arrest_rf_penalty, newdata = test_arrest, type = "prob")[,2]
train_arrest_ensemble$cart_ensemble <- predict(cart.penalty.ft, type = "prob")[,2]
test_arrest_ensemble$cart_ensemble <- predict(cart.penalty.ft, newdata = test_arrest, type = "prob")[,2]
train_arrest_ensemble$nb_ensemble <- predict(stops_arrest_nb_penalty, newdata = train_arrest, type = "raw")[,2]
test_arrest_ensemble$nb_ensemble <- predict(stops_arrest_nb_penalty, newdata = test_arrest, type = "raw")[,2]
# Renaming levels to avoid error
levels(train_arrest_ensemble$is_arrested) <- c("F", "T")
levels(test_arrest_ensemble$is_arrested) <- c("F", "T")

set.seed(123)
test_arrest_ensemble$rf_ensemble_class <- predict(stops_arrest_rf_penalty, newdata = test_arrest, type = "class")
test_arrest_ensemble$cart_ensemble_class <- predict(cart.penalty.ft, newdata = test_arrest, type = "class")
test_arrest_ensemble$nb_ensemble_class <- predict(stops_arrest_nb_penalty, newdata = test_arrest, type = "class")
levels(test_arrest_ensemble$rf_ensemble_class) <- c("F", "T")
levels(test_arrest_ensemble$cart_ensemble_class) <- c("F", "T")
levels(test_arrest_ensemble$nb_ensemble_class) <- c("F", "T")

test_arrest_ensemble$pred_maj <- as.factor(ifelse(test_arrest_ensemble$rf_ensemble_class == "T" & test_arrest_ensemble$cart_ensemble_class == "T", "T",                                                 ifelse(test_arrest_ensemble$rf_ensemble_class == "T" & test_arrest_ensemble$nb_ensemble_class == "T", "T",                                                        ifelse(test_arrest_ensemble$cart_ensemble_class == "T" & test_arrest_ensemble$nb_ensemble_class == "T", "T", "F"))))

confusionMatrix(table(test_arrest_ensemble$is_arrested, test_arrest_ensemble$pred_maj))$overall['Kappa']
roc.curve(test_arrest_ensemble$is_arrested, test_arrest_ensemble$pred_maj)
```
Again the Kappa and AUC are not much affected. Therefore, we will just stick with our CART with Penalty Matrix model, which is starred (*) above.

To fine-tune the chosen model a little bit further, we will adjust the threshold based off of the elbow of the ROC curve, finding the right balance between the false positive rate and the true positive rate.
```{r, message = FALSE}
set.seed(111)
final_model_pred <- predict(cart.penalty.ft, newdata = test_arrest, type = "p")
final_model_ROC <-  prediction(final_model_pred[,2], test_arrest$is_arrested)
ROCperf_finalmodel <- performance(final_model_ROC, "tpr", "fpr")
plot(ROCperf_finalmodel, colorize=TRUE, print.cutoffs.at=seq(0,0.1,by=0.01), text.adj=c(-0.2,1.7))
confusionMatrix(table(test_arrest$is_arrested, final_model_pred[,2] >= 0.065))$overall['Kappa']
roc.curve(test_arrest$is_arrested, final_model_pred[,2]>= 0.065)
```
This is our final model, leaving us with a Kappa of 0.2849794 and an AUC value of 0.720.

#### Conclusion:
The overall learnings from the machine learning investigation is that the dataset we currently have does not contain the most relevant information for successfully predicting arrest status. Testing different algorithms and combinations, fine-tuning parameters, and feature selecting don’t do much to improve our models, further cementing this point. However, we still learned some important information. The overall question of the project was to see if demographic factors play a role in arrest outcome of traffic stops. Because the important variables identified are not demographic and the fact that the models don’t perform well regardless of what the important variables were, the answer to our question seems to be that demographic factors don’t play a large role in arrest status. In fact, none of the features in the dataset play a large role. However, this doesn’t mean that the features don’t play any role. From our visual analysis, we saw statistically significant discrepancies in arrest status between different demographic populations. To gain a better-rounded and clearer understanding of what factors are at play in arrests and if implicit bias is present, more data is needed to establish a better performing model. 
